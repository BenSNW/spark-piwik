{"name":"Spark-piwik","tagline":"An integration of Piwik Analytics with Scala and Apache Spark","body":"![Dr.Krusche & Partner PartG](https://raw.github.com/skrusche63/spark-elastic/master/images/dr-kruscheundpartner.png)\r\n\r\n## Piwik Web Analytics meets Apache Spark \r\n\r\n> Transform customer engagement data from Piwik Analytics into actionable business insights.\r\n\r\nIn this project, we illustrate that Apache Spark not only is a fast and general engine for large-scale data processing, but also an appropriate means to integrate existing data sources and make their data \r\napplicable by sophisticated machine learning, mining and prediction algorithms. As a specific data source, we selected [Piwik Analytics](http://piwik.org/), which is a widely used open source platform for \r\nweb analytics, and, an appropriate starting point for market basket analysis, user behavior analytics and more.\r\n\r\nFrom [piwik.org](http://piwik.org/)\r\n> Piwik is the leading open source web analytics platform that gives you valuable insights into your websiteâ€™s visitors, your marketing campaigns and much more, so you can optimize your strategy and online experience of your visitors.\r\n\r\nIntegrating Piwik Analytics with Apache Kafka, Spark and other technologies from the Apache eco system enables to evaluate customer engagement data from Piwik with Association Rule & Frequent Sequence Mining, Context-Aware Recommendations, Markov Models and more to gain insights into customer engagement data far beyond traditional web analytics.\r\n\r\n![Apache Spark and Piwik Analytics](https://raw.githubusercontent.com/skrusche63/spark-piwik/master/images/Apache-Spark-and-Piwik.png)\r\n\r\n### Historical Engagement Data\r\n\r\n> Integration is based on Piwik's MySQL database.\r\n\r\nThe few lines of Scale code below show how to access customer engagement data persisted in Piwik's MySQL database. The connector requires the respective database location, name and user credentials. \r\n\r\nCustomer engagement data are retrieved by specifying the unique identifier `idsite` of a certain website supported by Piwik, and a specific query statement `query`.\r\n```\r\nobject MySQLConnector {\r\n\r\n  private val MYSQL_DRIVER   = \"com.mysql.jdbc.Driver\"\r\n  private val NUM_PARTITIONS = 1\r\n   \r\n  def readTable(sc:SparkContext,url:String,database:String,user:String,password:String,idsite:Int,query:String,fields:List[String]):RDD[Map[String,Any]] = {\r\n    \r\n    val result = new JdbcRDD(sc,() => getConnection(url,database,user,password),\r\n      query,idsite,idsite,NUM_PARTITIONS,\r\n      (rs:ResultSet) => getRow(rs,fields)\r\n    ).cache()\r\n\r\n    result\r\n    \r\n  }\r\n\r\n```\r\n\r\n#### Customer Segmentation\r\n\r\nGrouping customers or visitors of a business website by a common set of features such as time of engagement, location and others help marketers or publishers to reach a specific target audience.\r\n\r\nIn this project KMeans clustering is applied to customer transaction data from the `piwik_log_conversion` table. The focus is on the geo location of the customers or visitors who made the respective transactions. For more information please look into the `ClusterBuilder`object of this project.\r\n\r\nFrom these data a heatmap can be drawn to visualize from which region of world most of the transactions come from. Most of the functionality to create heatmaps is covered by the objects `HeatUtil`and `PixelUtil` of this project.\r\n\r\nThe image below shows a multi-colored heatmap from real data extracted from the `piwik_log_conversion`table. The colors red, yellow, green and blue indicate different heat ranges.\r\n\r\n![Heatmap from Piwik Data](https://raw.githubusercontent.com/skrusche63/spark-piwik/master/images/heatmap.png)\r\n\r\nSegmenting customers into specific target groups is not restricted their geo location. Time of the day, product or service categories, total revenue, and other parameters may directly be used to group customers by their purchase behavior.\r\n\r\n#### Purchase Horizon\r\n\r\nCustomers of products or service have a natural rhythm with respect to his or her purchase behavior. Regular visitors tend to visit an ecommerce store according to some temporal patterns that are inherent in their purchase history.\r\n\r\nEmail marketing or any other marketing campaign leads to better results if it will be aligned with these temporal patterns of the customers. This means that a marketing campaign that will go out at the right time for the right customer is obviously more effective than a campaign that does not take the customer's rhythm into account.\r\n\r\nPredicting the right purchase horizon for each customer requires to build a personalized predictive model from the \r\ncustomer's historical engagement data. Once this model is available, it is easy to predict when a certain customer is likely to make the next purchase and how much he or she is likely to spend.\r\n\r\nOptimizing email marketing is just one use case for predictive models. As Piwik Analytics tracks the ecommerce items being purchased with each order or transaction, the predicted purchase horizon may also be combined with Association Rule Mining (see below) to recommend products or services for time delayed cross-selling.\r\n\r\nIn this project, we use personalized [Markov Models](http://www.cs.sjsu.edu/faculty/stamp/RUA/HMM.pdf) to predict \r\nwhen a customer is likely to make the next purchase.\r\n\r\nThe idea behind this approach is to aggregate customer transaction data into discrete well-defined states. A timely ordered list of customer transaction may then be interpreted as a sequence of states, where each pair of subsequent states is accompanied by a state transition probability.\r\n\r\nWith this transition probability in mind, it is no magic to compute the next probable state.\r\n\r\nIn this project, we focus on `server_time` and `revenue_subtotal` from Piwik's `piwik_log_conversion` table, and represent a state by a two letter symbol:\r\n\r\n| Time since last transaction | Revenue compared to last transaction |\r\n| --- | --- |\r\n| S: Small | L: Significantly less |\r\n| M: Medium | E: More or less equal |\r\n| L: Large | G: Significantly greater |\r\n\r\nTransactions are then described as sequences of states:\r\n```\r\nidsite|idvisitor|state state state ...\r\n-----------------------------------------------------\r\n\r\n1|b65ce95de5c8e7ea|SG SL SG \r\n1|b65ce95de5c8e7ea|SL SG LL SG\r\n1|b65ce95de5c8e7ea|LL SG MG LL\r\n...\r\n\r\n```\r\nFrom this representation of customer transactions, we count the frequency of subsequent state pairs, i.e. `(SG,SL)` or `(LL,SG)`, normalize and scale these frequencies to finally end up with state transition probabilities.\r\n\r\nThe functionality described above is covered by the `MarkovBuilder`, that is also responsible for serializing and persisting the derived personalized predictive models.\r\n\r\nFinally, the `MarkovPredictor is responsible for predicting the next likely time and amount of transaction, using the personalized Markov Models as well as the last transactions of a certain customer. \r\n\r\n\r\n#### Customer Loyalty\r\n\r\nThe customer's engagement with a business is reflected by a wide range of events, such as e-commerce orders (or transactions), service calls, social media comments and more. All these events are indicative of the customer's loyalty to a certain business.\r\n\r\nLoyalty is usually defined as the strength of the relationship between a customer and a certain business. A higher degree of loyalty results in more purchase orders at a higher frequency.\r\n\r\nCustomer loyalty is an important parameter for almost any kind of business, and can e.g. be used to measure the effectiveness of marketing campaigns. Following a campaign, the loyalty curve can be analyzed for a certain period of time to see if there is any significant impact on customers loyalty.\r\n\r\nLoyalty, however, can not be directly observer and measured. It is an internal customer state, that must be inferred adn predicted from customer engagement events.\r\n\r\n> Customer Loyalty is a highly valuable business insight derived from customer engagement data using **Predictive Analytics**.\r\n\r\nWe suggest to predict a sequence of (hidden) customer loyalty states from a sequence of observed customer engagement data by using a [Hidden Markov Model](http://en.wikipedia.org/wiki/Hidden_Markov_model). Identifying customers with a downward loyalty curve with such analysis can directly trigger proactive actions to resurrect the relationships of these customers.\r\n\r\nIn the following, we make use of the functionality of the [Spark-HMM](https://github.com/skrusche63/scala-hmm) project.\r\n\r\n*To be continued*\r\n\r\n#### Cross-Selling and more \r\n\r\nAssociation rule mining is a wide-spread method to discover interesting relations between items in large-scale databases. These relations \r\nare specified as so called *association rules*. A very popular application area for association rules is the detection of regularities between \r\nproducts in large-scale customer engagement data recorded by ecommerce websites or point-of-sale systems in supermarkets.\r\n\r\nFor example, the rule [onions, potatoes] -> [burger] indicates that if a customer buys onions and potatoes together, he or she is likely to also buy \r\nhamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements. \r\n\r\nIn this project, we retrieve historical engagement data from the `piwik_log_conversion_item` table with Spark and transform these data into an appropriate transaction format. To this end, all ecommerce items that refer to the same ecommerce order are aggregated into single line.\r\n\r\n**Note**: The retrieved ecommerce items are filtered by those items that have not been deleted from a certain order. \r\n\r\nThe output of this transformation has the format specified here,\r\n```\r\nidsite|idvisitor|idorder|timestamp|item item item ...\r\n-----------------------------------------------------\r\n\r\n1|b65ce95de5c8e7ea|A10000124|1407986582000|1 2 4 5 \r\n1|b65ce95de5c8e7ea|A10000123|1407931845000|2 3 5\r\n1|b65ce95de5c8e7ea|A10000125|1407986689000|1 2 4 5\r\n...\r\n\r\n```\r\nand is done by the following lines of Scala code:\r\n```\r\ndef fromLogConversionItem(sc:SparkContext,idsite:Int,startdate:String,enddate:String):RDD[String] = {\r\n\r\n  val fields = LOG_ITEM_FIELDS  \r\n\r\n  val query = sql_logConversionItem.replace(\"$1\",startdate).replace(\"$2\",enddate)    \r\n  val rows = MySQLConnector.readTable(sc,url,database,user,password,idsite,query,fields)  \r\n    \r\n  val items = rows.filter(row => (isDeleted(row) == false)).map(row => {\r\n      \r\n    val idsite  = row(\"idsite\").asInstanceOf[Long]\r\n    val idvisitor = row(\"idvisitor\").asInstanceOf[Array[Byte]]     \r\n\r\n    val user = new java.math.BigInteger(1, idvisitor).toString(16)\r\n\r\n    val server_time = row(\"server_time\").asInstanceOf[java.sql.Timestamp]\r\n    val timestamp = server_time.getTime()\r\n      \r\n    val idorder = row(\"idorder\").asInstanceOf[String]      \r\n    val idaction_sku = row(\"idaction_sku\").asInstanceOf[Long]\r\n    \r\n    (idsite,user,idorder,idaction_sku,timestamp)\r\n    \r\n  })\r\n\r\n  items.groupBy(_._3).map(valu => {\r\n\r\n    val data = valu._2.toList.sortBy(_._5)      \r\n    val output = ArrayBuffer.empty[String]\r\n      \r\n    val (idsite,user,idorder,idaction_sku,timestamp) = data.head\r\n    output += idaction_sku.toString\r\n      \r\n    for (record <- data.tail) {\r\n      output += record._4.toString\r\n    }\r\n      \r\n    \"\" + idsite + \"|\" + user + \"|\" + idorder + \"|\" + timestamp + \"|\" + output.mkString(\" \")\r\n      \r\n  })\r\n    \r\n}\r\n```\r\nDiscovering the Top K Association Rules from the transactions above, does not require any reference to certain website (`idsite`), visitor (`idvisitor`) or order (`idorder`). It is sufficient to\r\nspecify all items of a transaction in a single line, and assign a unique line number (`lno`):\r\n\r\n```\r\nlno|item item item ...\r\n----------------------\r\n\r\n0,4 232 141 6\r\n1,169 129 16\r\n2,16 6 175 126\r\n3,16 124 141 175\r\n4,16 124 175\r\n5,232 4 238\r\n...\r\n```\r\n\r\nThe code below describes the `RuleBuilder` class that is responsible for discovering the association rules between the ecommerce items extracted from Piwik' database. \r\n\r\n**Note**: The `RuleBuilder`depends on the code base of the [Spark-ARULES](https://github.com/skrusche63/spark-arules) project.\r\n\r\n```\r\nclass RuleBuilder {\r\n\r\n  /**\r\n   * input = [\"idsite|user|idorder|timestamp|items\"]\r\n   */\r\n  def buildTopKRules(sc:SparkContext,dataset:RDD[String],k:Int=10,minconf:Double=0.8):String = {\r\n    \r\n    /* Prepare dataset */\r\n    val transactions = prepare(sc,dataset)\r\n    \r\n    /* Extract rules and convert into JSON */\r\n    val rules = TopK.extractRDDRules(sc,transactions,k,minconf)\r\n    TopK.rulesToJson(rules)\r\n     \r\n  }\r\n  \r\n  def prepare(sc:SparkContext,dataset:RDD[String]):RDD[(Int,Array[String])] = {\r\n\r\n    /* Reduce dataset to items and repartition to single partition */\r\n    val items = dataset.map(line => line.split(\"|\")(4)).coalesce(1)\r\n    \r\n    val index = sc.parallelize(Range.Long(0, items.count, 1),items.partitions.size)\r\n    val zip = items.zip(index) \r\n    \r\n    zip.map(valu => {\r\n      \r\n      val (line,no) = valu\r\n      (no.toInt, line.split(\" \"))\r\n      \r\n    })\r\n   \r\n  }\r\n\r\n} \r\n```\r\n\r\nThe table describes the result of the Top K Association Rule Mining, where `k = 10`  and the confidence threshold is set to `minconf = 0.8`. For example, the first row of table indicates that if a customers buys the ecommerce items with the `idaction_sku` value of `4` and `232` together, then there is a likelihood of `90%` that he or she also buys the item with the identifier `141`: [4, 232] -> [141].\r\n\r\n| antecedent  | consequent | support | confidence |\r\n| ------------- | ------------- |------------- | ------------- |\r\n| 4, 232        | 141  | 35 | 0.90 |\r\n| 169, 238      | 110  | 41 | 0.84 |\r\n| 6, 232        | 124  | 39 | 0.83 |\r\n| 129, 175      | 141  | 35 | 0.83 |\r\n| 124, 132, 175  | 141  | 35 | 0.83 | \r\n| 124, 126      | 132  | 37 | 0.82 |\r\n| 175, 232      | 124  | 38 | 0.81 |\r\n| 16, 124, 141   | 132  | 36 | 0.80 |\r\n| 16, 232       | 141  | 37 | 0.80 |\r\n| 16, 175       | 141  | 41 | 0.80 |\r\n\r\nFrom the association rules discovered it is no magic to support features such as\r\n\r\n> Customers who bought product A also bought product B and / or C\r\n\r\nor even recommendations. The use case described above for applying Association Rule Mining focuses on a single predicate, i.e. `buys`, and the items evaluated are solely ecommerce items from a web store. The customer engagement data available from Piwik's database, however, offer the opportunity to evaluate higher dimensional predicates.\r\n\r\nFrom the `piwik_log_conversion` table, we may easily extract further predicates, such as customer `location`, item `quantity` and more. Transforming these predicates into categorical variables (having a discrete set of possible values), such as `city = berlin` or `daytime = afternoon`, enables to assign additional item identifiers.\r\n\r\nTaking those additional predicates into account leads to more detailed association rules:\r\n\r\n> daytime(\"afternoon\") AND location(\"berlin\") AND buys(\"onions\") AND buys(\"tomatoes\") -> buys(\"burger\") \r\n\r\nWe have discussed association rules so far with respect to ecommerce events. The technique may - of course -  also be applied to pageview events. In this scenario, a transaction specifies a web session or visit, and an item characterizes a certain web page (url). \r\n\r\nInstead of retrieving customer engagement data from the `piwik_log_conversion_item` table, transactions are derived from the `piwik_log_link_visit_action` table.\r\n\r\n#### Customer Similarity\r\n\r\nCustomer engagement data from Piwik's `piwik_log_conversion_item` table is easily transformed into sequential data, where each customer order or transaction is aggregated into an itemset, and all those itemsets in a certain time window form a sequence of itemsets.\r\n\r\nThese sequences describe the temporally customer purchase behavior. This is the starting point for discovering and grouping users with similar purchase behavior.\r\n\r\nIn this project, we have published a `SimilarityBuilder`, which computes the purchase sequence similarities within a sequence database, and then determines the K most similar customers or visitors with respect to a selected customer.\r\n\r\nThe `SimilarityBuilder` starts with engagement data from the `piwik_log_conversion_item` table transformed into the following format (see above):\r\n\r\n```\r\nidsite|idvisitor|idorder|timestamp|item item item ...\r\n-----------------------------------------------------\r\n\r\n1|b65ce95de5c8e7ea|A10000124|1407986582000|1 2 4 5 \r\n1|b65ce95de5c8e7ea|A10000123|1407931845000|2 3 5\r\n1|b65ce95de5c8e7ea|A10000125|1407986689000|1 2 4 5\r\n...\r\n```\r\nFrom these data, it is only a few lines of Scala code to calculate a similarity matrix, which specifies the similarity of users `(i,j)` with respect to their purchase behavior:\r\n```\r\ndef build(sc:SparkContext,source:RDD[String],output:String) {\r\n\r\n  val dataset = source.map(line => {\r\n    \r\n    val Array(idsite,user,idorder,timestamp,items) = line.split(\"|\")\r\n    \r\n    val cid = idsite + \"|\" + user\r\n    (cid,idorder,timestamp.toLong,items)\r\n    \r\n  }).groupBy(_._2)\r\n    \r\n  val sequences = dataset.map(valu => {\r\n      \r\n    val records = valu._2.toList.sortBy(_._3)\r\n      \r\n    val cid = records.head._1\r\n    val sequence = records.map(_._4.split(\" \").map(_.toInt)).toArray\r\n      \r\n    (cid,sequence)\r\n      \r\n  })\r\n    \r\n  matrix = computeSimilarity(sequences)\r\n    \r\n}\r\n\r\n```\r\n\r\n\r\n---\r\n\r\n### Real-time Engagement Data\r\n\r\n> Integration is based on Spray, Apache Kafka and Piwik's tracking library (JavaScript).\r\n\r\nHistorical customer engagement data are an appropriate means to discover valuable customer insights, and also to build predictive models, that may then be applied to customer engagement in real-time. \r\n\r\nIn this project, we show how customer events sent by the Piwik tracking library (Javascript) \r\n\r\n* may directly be received by a reactive REST server, \r\n* delegated to a large-scale distributed message system, \r\n* consumed by Apache Spark and \r\n* finally evaluated by applying predictive models.\r\n\r\n*To be continued*\r\n\r\n#### Real-time Outlier Detection\r\n\r\n*To be done*\r\n\r\n---\r\n\r\n### Technology Stack\r\n\r\n* Akka\r\n* Apache Kafka\r\n* Apache Spark\r\n* Spray\r\n* MySQL\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}